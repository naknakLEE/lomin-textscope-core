version: "3.8"

services:
  serving:
    image: docker.lomin.ai/ts-gpu-serving-base:${TEXTSCOPE_BASE_IMAGE_VERSION}
    build:
      context: inference_server
      args:
        USER: ${USER}
        CUSTOMER: ${CUSTOMER}
        BUNDLE_PATH: ${BUNDLE_PATH}
        PYTHON_VERSION: 3.8
        MODEL_SERVICE: ${MODEL_SERVICE}
        UBUNTU_VERSION: ${UBUNTU_VERSION}
        POETRY_VERSION: ${POETRY_VERSION:-1.1.13}
        TEXTSCOPE_BASE_IMAGE_VERSION: ${TEXTSCOPE_BASE_IMAGE_VERSION}
    ports:
      - 5000:${SERVING_IP_PORT}
      - 5002:5002
      - 8266:8265
    volumes:
      - ./:/workspace
    shm_size: "16gb"
    ulimits:
      memlock: -1
      stack: 67108864
    environment:
      - COLUMNS=${COLUMNS:-200}
      - DISPLAY=$DISPLAY
      - NVIDIA_VISIBLE_DEVICES=all
      - TERM=$TERM
      - BENTOML_CONFIG=/workspace/inference_server/assets/bentoml_configuration.yml
      - WARMUP_COUNT=${WARMUP_COUNT:-0}
    tty: true
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["0"]
              capabilities: [gpu]
    working_dir: /workspace/inference_server
    entrypoint: sh assets/run-dev.sh

  web:
    image: docker.lomin.ai/ts-web-base:${TEXTSCOPE_BASE_IMAGE_VERSION}
    build:
      context: ./
      dockerfile: docker/base/Dockerfile.web
      args:
        PYTHON_VERSION: ${PYTHON_VERSION}
        UBUNTU_VERSION: ${UBUNTU_VERSION}
        POETRY_VERSION: ${POETRY_VERSION:-1.1.13}
        TEXTSCOPE_BASE_IMAGE_VERSION: ${TEXTSCOPE_BASE_IMAGE_VERSION}
    environment:
      - COLUMNS=${COLUMNS:-200}
    ports:
      - 8000:${WEB_IP_PORT}
      - 8267:8265
    volumes:
      - ./:/workspace
    tty: true
    working_dir: /workspace/app
    entrypoint: python3 main.py

  pp:
    image: docker.lomin.ai/ts-pp-base:${TEXTSCOPE_BASE_IMAGE_VERSION}
    build:
      context: ./
      dockerfile: docker/base/Dockerfile.pp
      args:
        PYTHON_VERSION: ${PYTHON_VERSION}
        UBUNTU_VERSION: ${UBUNTU_VERSION}
        POETRY_VERSION: ${POETRY_VERSION:-1.1.13}
        TEXTSCOPE_BASE_IMAGE_VERSION: ${TEXTSCOPE_BASE_IMAGE_VERSION}
    environment:
      - COLUMNS=${COLUMNS:-200}
    # ports:
    #   - 8080:${PP_IP_PORT}
    volumes:
      - ./:/workspace
    tty: true
    restart: unless-stopped
    working_dir: /workspace/pp_server/pp
    entrypoint: python3 main.py

  wrapper:
    image: docker.lomin.ai/ts-wrapper-base:${TEXTSCOPE_BASE_IMAGE_VERSION}
    build:
      context: ./
      dockerfile: docker/base/Dockerfile.wrapper
      args:
        UBUNTU_VERSION: ${UBUNTU_VERSION}
        POETRY_VERSION: ${POETRY_VERSION:-1.1.13}
        TEXTSCOPE_BASE_IMAGE_VERSION: ${TEXTSCOPE_BASE_IMAGE_VERSION}
        CUSTOMER: ${CUSTOMER}
    ports:
      - 8090:${WRAPPER_IP_PORT}
      - 11120:11120
    volumes:
      - ./:/workspace
    tty: true
    environment:
      - COLUMNS=${COLUMNS:-200}
      - API_ENV=production
      - PYTHONPATH=/workspace/textscope_wrapper
      - LANG=C.UTF-8
      - LC_ALL=C.UTF-8
      - CUSTOMER=${CUSTOMER}
    working_dir: /workspace/textscope_wrapper/wrapper
    entrypoint: python3 main.py

  nginx:
    ports:
      - 8050:80
      - 443:443

  nginx-exporter:
    ports:
      - 9113:9113

  gpu_telemetry:
    ports:
      - 9400:9400

  hardware_telemetry:
    ports:
      - 9100:9100

  prometheus:
    ports:
      - 9090:9090

  grafana:
    ports:
      - 3001:3000
