version: '3.8'

services:
  
  serving:
    image: docker.lomin.ai/ts-gpu-serving-base:${TEXTSCOPE_BASE_IMAGE_VERSION}
    build:
      context: ./
      dockerfile: docker/base/Dockerfile.gpu_serving
      args:
        USER: ${USER}
        CUSTOMER: ${CUSTOMER}
        BUNDLE_PATH: ${BUNDLE_PATH}
        PYTHON_VERSION: ${PYTHON_VERSION}
        MODEL_SERVICE: ${MODEL_SERVICE}
        UBUNTU_VERSION: ${UBUNTU_VERSION}
        BASE_PATH: ${BASE_PATH}
        LINUX_ENV_PATH: ${LINUX_ENV_PATH}
        DEMOMA_PATH: ${DEMOMA_PATH}
        SO_EXTENTION: ${SO_EXTENTION}
        TEXTSCOPE_BASE_IMAGE_VERSION: ${TEXTSCOPE_BASE_IMAGE_VERSION}
    ports: 
      - 5000:5000
      - 5002:5002
      - 8266:8265
    volumes:
      - ./:/workspace
    shm_size: "16gb"
    ulimits:
      memlock: -1
      stack: 67108864
    environment:
      - DISPLAY=$DISPLAY
      - NVIDIA_VISIBLE_DEVICES=all
      - TERM=$TERM
      - DEBUG=/workspace/inference_server/assets/bentoml_configuration.yml
      - BENTOML_CONFIG=/workspace/inference_server/assets/bentoml_configuration.yml
      - ARTIFACT_NAME_LIST=${ARTIFACT_NAME_LIST}
    tty: true
      
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            device_ids: ['0']
            capabilities: [gpu]
    working_dir: /workspace/inference_server
    command: bentoml serve-gunicorn ModelService/ --timeout=${MODEL_SERVER_TIMEOUT_SECOND}

  
  web:
    image: docker.lomin.ai/ts-web-base:${TEXTSCOPE_BASE_IMAGE_VERSION}
    build:
      context: ./
      dockerfile: docker/base/Dockerfile.web
      args:
        UBUNTU_VERSION: ${UBUNTU_VERSION}
        TEXTSCOPE_BASE_IMAGE_VERSION: ${TEXTSCOPE_BASE_IMAGE_VERSION}
    ports: 
      - 8000:8000
      - 8267:8265
    volumes:
      - ./:/workspace
    tty: true
    working_dir: /workspace/app
    # command: gunicorn main:app -w 4 -k uvicorn.workers.UvicornWorker -b 0:8000
    command: python3 main.py

  pp:
    image: docker.lomin.ai/ts-pp-base:${TEXTSCOPE_BASE_IMAGE_VERSION}
    build:
      context: ./
      dockerfile: docker/base/Dockerfile.pp
      args:
        UBUNTU_VERSION: ${UBUNTU_VERSION}
        TEXTSCOPE_BASE_IMAGE_VERSION: ${TEXTSCOPE_BASE_IMAGE_VERSION}
    ports: 
      - 8080:8080
    volumes:
      - ./:/workspace
    tty: true
    restart: unless-stopped
    working_dir: /workspace/pp_server/pp_server/app
    # command: gunicorn main:app -w 9 -k uvicorn.workers.UvicornWorker -b 0:8080
    command: python3 main.py

  wrapper:
    image: docker.lomin.ai/ts-wrapper-base:${TEXTSCOPE_BASE_IMAGE_VERSION}
    build:
      context: ./
      dockerfile: docker/base/Dockerfile.wrapper
      args:
        UBUNTU_VERSION: ${UBUNTU_VERSION}
        TEXTSCOPE_BASE_IMAGE_VERSION: ${TEXTSCOPE_BASE_IMAGE_VERSION}
        CUSTOMER: ${CUSTOMER}
    ports: 
      - 8090:8090
      - 11120:11120
    volumes:
      - ./:/workspace
    tty: true
    environment: 
      - API_ENV=production
      - PYTHONPATH=/workspace/textscope_wrapper
      - LANG=C.UTF-8
      - LC_ALL=C.UTF-8
      - CUSTOMER=${CUSTOMER}
    working_dir: /workspace/textscope_wrapper/textscope_wrapper
    # command: gunicorn main:app -w 4 -k uvicorn.workers.UvicornWorker -b 0:8090
    command: python3 main.py

  # nginx:
  #   ports:
  #     - 8050:80
  #     - 443:443
    
  # nginx-exporter:
  #   ports:
  #     - 9113:9113

  gpu_telemetry:
    ports:
      - 9400:9400

  hardware_telemetry:
    ports:
      - 9100:9100

  prometheus:
    ports:
      - 9090:9090
    
  grafana:
    ports:
      - 3000:3000
