version: "2.3"

services:
  serving:
    image: docker.lomin.ai/ts-gpu-serving-base:${TEXTSCOPE_BASE_IMAGE_VERSION}
    container_name: serving
    tty: true
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - CUSTOMER=${CUSTOMER}
      - WARMUP_COUNT=${WARMUP_COUNT:-0}
    runtime: nvidia
    build:
      context: inference_server
      args:
        USER: ${USER}
        CUSTOMER: ${CUSTOMER}
        PYTHON_VERSION: 3.8
        BUNDLE_PATH: ${BUNDLE_PATH}
        UBUNTU_VERSION: ${UBUNTU_VERSION}
        TEXTSCOPE_BASE_IMAGE_VERSION: ${TEXTSCOPE_BASE_IMAGE_VERSION}
        POETRY_VERSION: ${POETRY_VERSION}
        MODEL_SERVICE: ${MODEL_SERVICE}
    volumes:
      - ./:/workspace
    entrypoint: bash

  web:
    image: docker.lomin.ai/ts-web-base:${TEXTSCOPE_BASE_IMAGE_VERSION}
    container_name: web
    environment:
      - CUSTOMER=${CUSTOMER}
    build:
      context: ./
      dockerfile: docker/base/Dockerfile.web
      args:
        UBUNTU_VERSION: ${UBUNTU_VERSION}
        POETRY_VERSION: ${POETRY_VERSION}
        PYTHON_VERSION: ${PYTHON_VERSION}
    tty: true
    volumes:
      - ./:/workspace
    entrypoint: bash

  pp:
    image: docker.lomin.ai/ts-pp-base:${TEXTSCOPE_BASE_IMAGE_VERSION}
    container_name: pp
    environment:
      - CUSTOMER=${CUSTOMER}
    tty: true
    build:
      context: ./
      dockerfile: docker/base/Dockerfile.pp
      args:
        UBUNTU_VERSION: ${UBUNTU_VERSION}
        POETRY_VERSION: ${POETRY_VERSION:-1.1.13}
        PYTHON_VERSION: ${PYTHON_VERSION}
    volumes:
      - ./:/workspace
    entrypoint: bash
