ARG UBUNTU_VERSION

FROM nvidia/cuda:11.1-cudnn8-runtime-ubuntu${UBUNTU_VERSION}

ARG BUILD_FOLDER_PATH
ARG APP_NAME
ARG CUSTOMER
ARG PYTHON_VERSION
ARG MAINTAINER

LABEL maintainer=${MAINTAINER}

ENV LC_ALL=C.UTF-8
ENV LANG=C.UTF-8
ENV PYTHONPATH="$PYTHONPATH:/workspace/${APP_NAME}"
ENV API_ENV="production"

RUN apt-get update && \
    apt-get install -y locales && \
    locale-gen ko_KR.UTF-8 && \
    apt-get -y install python3-pip && \
    apt-get install -y git  && \
    DEBIAN_FRONTEND="noninteractive" apt-get -y install tzdata && \
    apt-get -y install libgl1-mesa-glx libglib2.0-0 && \
    apt-get -y install poppler-utils &&

RUN pip3 install --upgrade pip && \
    pip3 install torch==1.9.0+cu111 torchvision==0.10.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html

RUN git clone http://github.com/bentoml/bentoml && \
    cd bentoml && \
    pip3 install -r ./guides/quick-start/requirements.txt

COPY ./requirments/requirments-serving.txt /workspace/
RUN pip3 install -r /workspace/requirments-serving.txt && \
    sed -i 's/task=task.to_json(),/task=InferenceTask(**{key: getattr(task, key) for key in task.__dict__ if key != "data" }),/' /usr/local/lib/python3.6/dist-packages/bentoml/service/inference_api.py

COPY ./${APP_NAME}/assets/bentoml-for-health-check /workspace/bentoml-for-health-check
RUN cp -r /workspace/bentoml-for-health-check/* /usr/local/lib/python${PYTHON_VERSION}/dist-packages/bentoml/

RUN rm /usr/local/lib/python3.6/dist-packages/bentoml/frameworks/pytorch.py && \
    rm /usr/local/lib/python3.6/dist-packages/bentoml/frameworks/onnx.py && \
    rm /usr/local/lib/python${PYTHON_VERSION}/dist-packages/bentoml/saved_bundle/pip_pkg.py && \
    rm /usr/local/lib/python${PYTHON_VERSION}/dist-packages/bentoml/saved_bundle/loader.py

COPY ./.env /workspace/
COPY ./${APP_NAME}/assets/textscope_${CUSTOMER}.json /workspace/${APP_NAME}/assets/
COPY ./${APP_NAME}/assets/bentoml_configuration.yml /workspace/${APP_NAME}/assets/
COPY ./${APP_NAME}/assets/gulim.ttc /workspace/${APP_NAME}/assets/
COPY ./${APP_NAME}/assets/document_understanding/* /workspace/inference_server/assets/document_understanding/tokenizer/
COPY ./${APP_NAME}/assets/modified_bentoml_file/pip_pkg.py /usr/local/lib/python${PYTHON_VERSION}/dist-packages/bentoml/saved_bundle/
COPY ./${APP_NAME}/assets/modified_bentoml_file/${CUSTOMER}_loader.py /usr/local/lib/python${PYTHON_VERSION}/dist-packages/bentoml/saved_bundle/loader.py
COPY ./${BUILD_FOLDER_PATH}/${CUSTOMER}-build/serving/CopiedModelService/ /workspace/${APP_NAME}/ModelService/
COPY ./${BUILD_FOLDER_PATH}/${CUSTOMER}-build/serving/frameworks/* /usr/local/lib/python3.6/dist-packages/bentoml/frameworks/
COPY ./${BUILD_FOLDER_PATH}/${CUSTOMER}-build/lovit/* /workspace/${APP_NAME}/
COPY ./${BUILD_FOLDER_PATH}/${CUSTOMER}-build/assets/* /workspace/assets/

RUN rm -rf /var/lib/apt/lists/* && \
    rm -rf /root/.cache /bentoml && \
    rm -rf /usr/bin/gcc /usr/bin/ssh

WORKDIR /

# ENTRYPOINT ["BENTOML_CONFIG=/workspace/${APP_NAME}/assets/bentoml_configuration.yml", "bentoml", "serve-gunicorn", "."]
