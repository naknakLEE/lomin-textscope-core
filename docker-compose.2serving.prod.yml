version: "3.8"

services:
  nginx_serving:
    image: docker.lomin.ai/ts-nginx-serving:${TEXTSCOPE_NGINX_VERSION}
    container_name: textscope-nginx-serving
    build:
      context: ./
      dockerfile: docker/production/Dockerfile.nginx-serving
    ports:
      - 50033:5000
    depends_on:
      - serving_0
      - serving_1
    networks:
      - our_net
      
  serving_0:
    container_name: textscope-serving_0
    build:
      context: inference_server
      dockerfile: ./Dockerfile.production
      args:
        MAINTAINER: ${MAINTAINER}
        APP_NAME: inference_server
        BUILD_FOLDER_PATH: ${BUILD_FOLDER_PATH}
        UBUNTU_VERSION: ${UBUNTU_VERSION}
        POETRY_VERSION: ${POETRY_VERSION}
        CUSTOMER: ${CUSTOMER}
        PYTHON_VERSION: ${PYTHON_VERSION}
        TEXTSCOPE_BASE_IMAGE_VERSION: ${TEXTSCOPE_BASE_IMAGE_VERSION}
    ports:
      - 8052:5000
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - SERVICE_INSTANCE=serving
      - WARMUP_COUNT=${WARMUP_COUNT:-30}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["0"]
              capabilities: [gpu]
    tty: true
    entrypoint: |
      sh -c "
                bentoml serve textscope_model_service:`cat /workspace/inference_server/assets/conf/service/default.yaml | shyaml get-value version` --host 0.0.0.0
            "

  serving_1:
    container_name: textscope-serving_1
    build:
      context: inference_server
      dockerfile: ./Dockerfile.production
      args:
        MAINTAINER: ${MAINTAINER}
        APP_NAME: inference_server
        BUILD_FOLDER_PATH: ${BUILD_FOLDER_PATH}
        UBUNTU_VERSION: ${UBUNTU_VERSION}
        POETRY_VERSION: ${POETRY_VERSION}
        CUSTOMER: ${CUSTOMER}
        PYTHON_VERSION: ${PYTHON_VERSION}
        TEXTSCOPE_BASE_IMAGE_VERSION: ${TEXTSCOPE_BASE_IMAGE_VERSION}
    ports:
      - 8052:5000
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - SERVICE_INSTANCE=serving
      - WARMUP_COUNT=${WARMUP_COUNT:-30}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["1"]
              capabilities: [gpu]
    tty: true
    entrypoint: |
      sh -c "
                bentoml serve textscope_model_service:`cat /workspace/inference_server/assets/conf/service/default.yaml | shyaml get-value version` --host 0.0.0.0
            "

  web:
    user: "0:0"
    build:
      context: ./
      dockerfile: docker/production/Dockerfile.web
      args:
        MAINTAINER: ${MAINTAINER}
        UBUNTU_VERSION: ${UBUNTU_VERSION}
        POETRY_VERSION: ${POETRY_VERSION}
        BUILD_FOLDER_PATH: ${BUILD_FOLDER_PATH}
        CUSTOMER: ${CUSTOMER}
        TEXTSCOPE_BASE_IMAGE_VERSION: ${TEXTSCOPE_BASE_IMAGE_VERSION}
    tty: true
    working_dir: /workspace
    # entrypoint: python3 main.py    
    entrypoint: gunicorn main:app -w 1 -k uvicorn.workers.UvicornWorker -b 0:${WEB_IP_PORT} --timeout 1200

  pp:
    user: "0:0"
    build:
      context: ./
      dockerfile: docker/production/Dockerfile.pp
      args:
        MAINTAINER: ${MAINTAINER}
        UBUNTU_VERSION: ${UBUNTU_VERSION}
        POETRY_VERSION: ${POETRY_VERSION}
        BUILD_FOLDER_PATH: ${BUILD_FOLDER_PATH}
        CUSTOMER: ${CUSTOMER}
        TEXTSCOPE_BASE_IMAGE_VERSION: ${TEXTSCOPE_BASE_IMAGE_VERSION}
    environment:
      - PYTHONPATH=/workspace
    tty: true
    working_dir: /workspace/pp_server
    # entrypoint: python3 main.py
    entrypoint: gunicorn main:app -w 1 -k uvicorn.workers.UvicornWorker -b 0:${PP_IP_PORT} --timeout 1200

  wrapper:
    user: "0:0"
    build:
      context: ./
      dockerfile: docker/production/Dockerfile.wrapper
      args:
        MAINTAINER: ${MAINTAINER}
        CUSTOMER: ${CUSTOMER}
        BUILD_FOLDER_PATH: ${BUILD_FOLDER_PATH}
        UBUNTU_VERSION: ${UBUNTU_VERSION}
        POETRY_VERSION: ${POETRY_VERSION}
        TEXTSCOPE_BASE_IMAGE_VERSION: ${TEXTSCOPE_BASE_IMAGE_VERSION}
    ports:
      - 8050:${WRAPPER_IP_PORT}
    tty: true
    working_dir: /workspace/${CUSTOMER}_wrapper/wrapper
    entrypoint: gunicorn main:app -w 1 -k uvicorn.workers.UvicornWorker -b 0:${WRAPPER_IP_PORT} --timeout 1200

  grafana:
    ports:
      - 8053:3000
